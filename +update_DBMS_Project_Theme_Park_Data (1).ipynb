{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DBMS Application\n",
        "\n",
        "Here is a Python DBMS application abiding by the restrictions set in the COSC 5510 Project 3 Instructions."
      ],
      "metadata": {
        "id": "ackQ7h6dTbjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README:\n",
        "Just run the notebook from top to bottom and all the necessary packages should be installed!\n",
        "\n",
        "Make sure that the relevant csv files are located in the same folder as the notebook, or if using Google Colab then dragged into the file storage area. (relation1, relation2, relation3, relation4, vistors, rides)\n",
        "\n",
        "There are example queries at the end of the document to run. To exit the program run \"exit;\""
      ],
      "metadata": {
        "id": "0opvpdMtuQNo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHMKaXCEXb1P"
      },
      "source": [
        "# Package setup\n",
        "[all packages have been approved by TAs in writing]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX9-uockXQMY",
        "outputId": "16326283-9f90-4743-b321-a4a7f1345d86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting btrees\n",
            "  Downloading BTrees-5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting persistent>=4.1.0 (from btrees)\n",
            "  Downloading persistent-5.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.4/231.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zope.interface>=5.0.0 (from btrees)\n",
            "  Downloading zope.interface-6.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from persistent>=4.1.0->btrees) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface>=5.0.0->btrees) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->persistent>=4.1.0->btrees) (2.21)\n",
            "Installing collected packages: zope.interface, persistent, btrees\n",
            "Successfully installed btrees-5.2 persistent-5.2 zope.interface-6.3\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages for this project\n",
        "# input and output formatting\n",
        "import csv\n",
        "from tabulate import tabulate\n",
        "from collections import defaultdict\n",
        "\n",
        "# indexing\n",
        "!pip install btrees\n",
        "from BTrees.OOBTree import OOBTree\n",
        "\n",
        "# execution time\n",
        "import time\n",
        "\n",
        "# Packages for parsing and command line interface\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import time\n",
        "import sqlparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYcG2djuX2mx"
      },
      "source": [
        "# Constant Datapaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFQGajiFXxjw"
      },
      "outputs": [],
      "source": [
        "DATA_FILE_PATH1 = 'relation1.csv'\n",
        "DATA_FILE_PATH2 = 'relation2.csv'\n",
        "DATA_FILE_PATH3 = 'relation3.csv'\n",
        "DATA_FILE_PATH4 = 'relation4.csv'\n",
        "DATA_FILE_PATH5 = 'visitors.csv'\n",
        "DATA_FILE_PATH6 = 'rides.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMbzPBVuYtIu"
      },
      "source": [
        "# Global Vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXCfzf48YsaL"
      },
      "outputs": [],
      "source": [
        "db = {} # DON'T REMOVE - HAS TABLE NAMES AND COLUMNS FOR DATABASE\n",
        "keys = {}\n",
        "file_paths = {\n",
        "    'data_1': 'relation1.csv',\n",
        "    'data_2': 'relation2.csv',\n",
        "    'data_3': 'relation3.csv',\n",
        "    'data_4': 'relation4.csv',\n",
        "    'visitors': 'visitors.csv',\n",
        "    'rides': 'rides.csv'\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlQjW3GIYVTo"
      },
      "source": [
        "# Loading in datasets from CSV Files\n",
        "Can be updated to bulk upload."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHXnjjNsZzbR"
      },
      "outputs": [],
      "source": [
        "def load_data(filename, db_name):\n",
        "    with open(filename, mode='r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        headers = next(reader)  # Extracting column names\n",
        "        data = [row for row in reader]\n",
        "\n",
        "    return {db_name: {'headers': headers, 'data': data}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYLOJ_I3YbsS"
      },
      "outputs": [],
      "source": [
        "'''def given_sample_upload():\n",
        "\n",
        "    # Loading the data from CSVs to our program\n",
        "    db_1 = load_data(DATA_FILE_PATH1, \"data_1\")\n",
        "    db_2 = load_data(DATA_FILE_PATH2, \"data_2\")\n",
        "    db_3 = load_data(DATA_FILE_PATH2, \"data_3\")\n",
        "    db_4 = load_data(DATA_FILE_PATH2, \"data_4\")\n",
        "    file_paths = {\n",
        "    'data_1': 'relation1.csv',\n",
        "    'data_2': 'relation2.csv',\n",
        "    'data_3': 'relation3.csv',\n",
        "    'data_4': 'relation4.csv',\n",
        "    }\n",
        "\n",
        "    # Combining databases into one dictionary for easier access\n",
        "    databases = {**db_1, **db_2, **db_3, **db_4}\n",
        "\n",
        "    # Adding tables to db - DON'T REMOVE THIS\n",
        "    db['data_1'] = ['col1', 'col2']\n",
        "    db['data_2'] = ['col1', 'col2']\n",
        "    db['data_3'] = ['col1', 'col2']\n",
        "    db['data_4'] = ['col1', 'col2']\n",
        "\n",
        "    # Displaying the data with headers for each dataset\n",
        "    for db_name, contents in databases.items():\n",
        "        print(f\"\\n{db_name}:\")\n",
        "        # Slicing the data to display only the first 10 rows\n",
        "        first_10_rows = contents['data'][:10]\n",
        "        print(tabulate(first_10_rows, headers=contents['headers'], tablefmt=\"grid\"))\n",
        "\n",
        "    return databases, file_paths\n",
        "'''\n",
        "\n",
        "def create_data(rows, pattern=1):\n",
        "  # Pattern - 1 : [[1, 1], [2, 2], ... ]\n",
        "  # Pattern - 2 : [[1, 1], [2, 1], ... ]\n",
        "  return [[i, i if pattern == 1 else 1] for i in range(1, rows + 1)]\n",
        "\n",
        "def given_sample_upload():\n",
        "  # Creating tables with headers and data\n",
        "  d1 = {'headers': ['col1', 'col2'], 'data': create_data(1000)}\n",
        "  d2 = {'headers': ['col1', 'col2'], 'data': create_data(1000, pattern=2)}\n",
        "  d3 = {'headers': ['col1', 'col2'], 'data': create_data(10000, pattern=1)}\n",
        "  d4 = {'headers': ['col1', 'col2'], 'data': create_data(10000, pattern=2)}\n",
        "  # d5 = load_data(DATA_FILE_PATH5, \"visitors\")\n",
        "  # d6 = load_data(DATA_FILE_PATH6, \"rides\")\n",
        "  keys['data_1'] = {'pk': 'col1', 'fk': ''}\n",
        "  keys['data_2'] = {'pk': 'col1', 'fk': ''}\n",
        "  keys['data_3'] = {'pk': 'col1', 'fk': ''}\n",
        "  keys['data_4'] = {'pk': 'col1', 'fk': ''}\n",
        "  keys['visitors'] = {'pk': 'visitor_id', 'fk': ''}\n",
        "  keys['rides'] = {'pk': 'ride_id', 'fk': 'visitor_id'}\n",
        "  # Combining databases into one dictionary for easier access\n",
        "  databases = {'data_1': d1, 'data_2': d2, 'data_3': d3, 'data_4': d4}\n",
        "  # Adding tables to db - DON'T REMOVE THIS\n",
        "  db['data_1'] = ['col1', 'col2']\n",
        "  db['data_2'] = ['col1', 'col2']\n",
        "  db['data_3'] = ['col1', 'col2']\n",
        "  db['data_4'] = ['col1', 'col2']\n",
        "  # db['visitors'] = ['visitor_id', 'name', 'age', 'favourite_character']\n",
        "  # db['rides'] = ['ride_id', 'visitor_id', 'ride_name', 'times_ridden']\n",
        "  # Displaying the data with headers for each dataset\n",
        "  '''for db_name, contents in databases.items():\n",
        "    print(f\"\\n{db_name}:\")\n",
        "    # Slicing the data to display only the first 10 rows\n",
        "    first_10_rows = contents['data'][:10]\n",
        "    print(tabulate(first_10_rows, headers=contents['headers'], tablefmt=\"grid\"))'''\n",
        "  return databases, file_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXplI598YfVs"
      },
      "outputs": [],
      "source": [
        "databases, file_paths = given_sample_upload()\n",
        "# output will be first 10 rows of each database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVKR0A5ArkCI"
      },
      "source": [
        "# Parsing Setup Code\n",
        "\n",
        "Functions all related to parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6endCWvroE4"
      },
      "outputs": [],
      "source": [
        "# A function to get token positions in the parsed query\n",
        "def positions(command):\n",
        "  pos = {}\n",
        "  cmd = str(command[0]).split()\n",
        "  keys = {\n",
        "        'select', 'distinct', 'as', 'count', 'max', 'min', 'avg', 'sum', 'from',\n",
        "        'right', 'left', 'join', 'on', 'where', 'and', 'or', 'not', 'in', 'between',\n",
        "        'like', 'group', 'having', 'order', 'limit', 'create', 'table', 'if', 'exists',\n",
        "        'insert', 'into', 'values', 'update', 'set', 'delete', 'drop', 'column', 'table', 'alter',\n",
        "        'add'\n",
        "        }\n",
        "  pos = {k: i for i, k in enumerate(cmd) if k in keys or k == ';'}\n",
        "  # print(pos)\n",
        "  return pos\n",
        "\n",
        "# A function to preprocess and clean the parsed query\n",
        "def clean(values):\n",
        "  value = []\n",
        "  chars_to_remove = \"();',\"\n",
        "  for v in values:\n",
        "    val = ''.join(char for char in v if char not in chars_to_remove).strip()\n",
        "    value.append(val)\n",
        "  return value\n",
        "\n",
        "# A function to convert data types of parsed query\n",
        "def format(fields):\n",
        "    result = {}\n",
        "    data_type = {'int': 'int', 'text': 'string'}\n",
        "    for i in range(0, len(fields), 2):\n",
        "        key = fields[i]\n",
        "        original_value = fields[i + 1]\n",
        "        normalized_value = re.sub(r'\\d+|\\(.*\\)', '', original_value).strip()\n",
        "        if 'char' in normalized_value or 'varchar' in normalized_value:\n",
        "            value = 'string'\n",
        "        else:\n",
        "            value = data_type.get(normalized_value, original_value)\n",
        "        result[key] = value\n",
        "    return result\n",
        "\n",
        "# A function to convert lists to dictionaries for manipulating data\n",
        "def format_values(columns, values):\n",
        "   c = len(columns)\n",
        "   v = len(values)\n",
        "   result = {}\n",
        "   if c == v:\n",
        "    for i in range(0,c):\n",
        "      result[columns[i]] = values[i]\n",
        "    return result\n",
        "   else:\n",
        "    print('Missing fields')\n",
        "    return None\n",
        "\n",
        "# A function to check duplicate values\n",
        "def duplicates(columns):\n",
        "  cols = []\n",
        "  d = False\n",
        "  for i in columns:\n",
        "    if i not in cols:\n",
        "      cols.append(i)\n",
        "    else:\n",
        "      d = True\n",
        "  return d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMecgZuJa3MF"
      },
      "source": [
        "# Indexing using BTrees\n",
        "Currently indexing entire dataset. Can change to just PKs.\n",
        "Includes search, update and delete.\n",
        "\n",
        "***need to add updating the file paths when these changes occur***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DynamicDB:\n",
        "    def __init__(self, databases):\n",
        "        self.datasets = databases\n",
        "        self.indices = {}\n",
        "\n",
        "    def load_indices(self):\n",
        "        \"\"\"Load indices for each dataset based on a predefined primary key for each dataset.\"\"\"\n",
        "        for db_name, contents in self.datasets.items():\n",
        "            headers = contents['headers']\n",
        "            pk = keys[db_name]['pk']  # Assuming 'keys' dict defines primary keys\n",
        "            self.create_indices(db_name, contents['data'], headers, [pk])\n",
        "\n",
        "    def create_indices2(self, dataset_id, data, headers, index_fields):\n",
        "        if dataset_id not in self.indices:\n",
        "            self.indices[dataset_id] = {}\n",
        "        for field in index_fields:\n",
        "            if field not in self.indices[dataset_id]:\n",
        "                self.indices[dataset_id][field] = OOBTree()\n",
        "            for i, row in enumerate(data):\n",
        "                key = row[headers.index(field)]\n",
        "                if key not in self.indices[dataset_id][field]:\n",
        "                    self.indices[dataset_id][field][key] = []\n",
        "                self.indices[dataset_id][field][key].append(i)  # Store indices of rows\n",
        "\n",
        "    def create_indices(self, dataset_id, data, headers, index_fields):\n",
        "      if dataset_id not in self.indices:\n",
        "        self.indices[dataset_id] = {}\n",
        "      for field in index_fields:\n",
        "        if field not in self.indices[dataset_id]:\n",
        "            self.indices[dataset_id][field] = OOBTree()\n",
        "        for i, row in enumerate(data):\n",
        "            key = str(row[headers.index(field)])  # Cast key to string to avoid type issues\n",
        "            if key not in self.indices[dataset_id][field]:\n",
        "                self.indices[dataset_id][field][key] = []\n",
        "            self.indices[dataset_id][field][key].append(i)\n",
        "\n",
        "\n",
        "    def check_index(self, dataset_id, field):\n",
        "        \"\"\"Ensure the field is indexed, index on-the-fly if it's not.\"\"\"\n",
        "        if dataset_id in self.datasets and dataset_id not in self.indices:\n",
        "            headers = self.datasets[dataset_id]['headers']\n",
        "            self.create_indices(dataset_id, self.datasets[dataset_id]['data'], headers, [field])\n",
        "        elif field not in self.indices.get(dataset_id, {}):\n",
        "            print(f\"Indexing field '{field}' because it was needed.\")\n",
        "            self.create_indices(dataset_id, self.datasets[dataset_id]['data'], self.datasets[dataset_id]['headers'], [field])\n",
        "\n",
        "    def query_by_index2(self, dataset_id, field, value):\n",
        "        \"\"\"Query rows based on index if the column is indexed.\"\"\"\n",
        "        self.check_index(dataset_id, field)  # Ensure index exists before querying\n",
        "        if field in self.indices.get(dataset_id, {}):\n",
        "            indices = self.indices[dataset_id][field].get(value, [])\n",
        "            return [self.datasets[dataset_id]['data'][i] for i in indices]\n",
        "        else:\n",
        "            print(f\"No index found for {field} with value {value}.\")\n",
        "            return []\n",
        "\n",
        "    def query_by_index(self, dataset_id, field, value):\n",
        "      \"\"\"Query rows based on index if the column is indexed.\"\"\"\n",
        "      self.check_index(dataset_id, field)  # Ensure index exists before querying\n",
        "      if field in self.indices.get(dataset_id, {}):\n",
        "        indices = self.indices[dataset_id][field].get(value, [])\n",
        "        # Flatten the list if it's a list of lists (which should not be the case ideally)\n",
        "        # This is just a safeguard; your actual implementation should not require this.\n",
        "        if indices and isinstance(indices[0], list):\n",
        "            indices = [item for sublist in indices for item in sublist]\n",
        "        return indices\n",
        "      else:\n",
        "        print(f\"No index found for {field} with value {value}.\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "yc4OQGgSQD7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ac6H8Pkb5tu"
      },
      "source": [
        "# Query Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLaI2IPmb9Ei"
      },
      "source": [
        "Print a specfic table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhf-kw1_cEi4"
      },
      "outputs": [],
      "source": [
        "def print_table(db_dict, table_name):\n",
        "    if table_name in db_dict:\n",
        "        table_data = db_dict[table_name]\n",
        "        print(f\"\\n{table_name}:\")\n",
        "        print(tabulate(table_data['data'], headers=table_data['headers'], tablefmt=\"grid\"))\n",
        "    else:\n",
        "        print(f\"Table '{table_name}' not found.\")\n",
        "\n",
        "# eg\n",
        "# print_table(databases, \"Data_1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFyxed45eajg"
      },
      "source": [
        "# Update function\n",
        "[Not indexed]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "MRIvLVQYeaOc",
        "outputId": "2fec389a-74eb-49ea-a67c-642262435ab6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# example usage\\n# Update the database\\ndb_name = \\'Data_1\\'\\nrow_index = 1  # Second row\\ncolumn_index = 1  # Second column (zero-indexed)\\nnew_value = \\'9\\'  # New value to set\\n\\n# Assuming all_databases has been defined and loaded as per our earlier discussions\\n# Since column names aren\\'t specified, we use indices; modify the update function accordingly if needed\\nsuccess = update_database(databases, db_name, row_index, column_index, new_value)\\nif success:\\n    print(\"Update successful.\")\\nelse:\\n    print(\"Update failed.\")\\nprint_table(databases, \"Data_1\") '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "def update_database(databases, db_name, row_index, column_index, new_value):\n",
        "\n",
        "    if db_name in databases:\n",
        "        database = databases[db_name]\n",
        "        data = database['data']\n",
        "\n",
        "        if row_index < len(data) and column_index < len(data[row_index]):\n",
        "            data[row_index][column_index] = new_value\n",
        "            print(f\"Updated '{db_name}' at row {row_index}, column {column_index} with new value: {new_value}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Error: Row index or column index out of range.\")\n",
        "            return False\n",
        "    else:\n",
        "        print(f\"Error: Database named '{db_name}' not found.\")\n",
        "        return False\n",
        "\n",
        "\"\"\"\n",
        "# example usage\n",
        "# Update the database\n",
        "db_name = 'Data_1'\n",
        "row_index = 1  # Second row\n",
        "column_index = 1  # Second column (zero-indexed)\n",
        "new_value = '9'  # New value to set\n",
        "\n",
        "# Assuming all_databases has been defined and loaded as per our earlier discussions\n",
        "# Since column names aren't specified, we use indices; modify the update function accordingly if needed\n",
        "success = update_database(databases, db_name, row_index, column_index, new_value)\n",
        "if success:\n",
        "    print(\"Update successful.\")\n",
        "else:\n",
        "    print(\"Update failed.\")\n",
        "print_table(databases, \"Data_1\") \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUJQz7iKsdW2"
      },
      "source": [
        "# Update Parser Code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dynamic_db = DynamicDB(databases);\n",
        "dynamic_db.load_indices()  # Load or reload indices to ensure all are set\n",
        "\n",
        "def update_data_archive(database, dynamic_db, table_name, setcol, setval, col, val):\n",
        "    # Check if the table exists in the database\n",
        "    if table_name not in database:\n",
        "        print(f\"Table '{table_name}' not found.\")\n",
        "        return\n",
        "\n",
        "    # Perform a query using indices\n",
        "    dynamic_db.check_index(table_name, col)\n",
        "    matched_indices = dynamic_db.query_by_index(table_name, col, val)\n",
        "    if not matched_indices:\n",
        "        print(f\"No records found where {col} = {val}.\")\n",
        "        return\n",
        "\n",
        "    # Retrieve table data\n",
        "    table = database[table_name]\n",
        "    headers = table['headers']\n",
        "    data = table['data']\n",
        "\n",
        "    # Find the indices of the columns\n",
        "    setcol_index = headers.index(setcol)\n",
        "    col_index = headers.index(col)\n",
        "\n",
        "    # Update the rows that match the condition\n",
        "    updated_rows = 0\n",
        "    for index in matched_indices:\n",
        "        row = data[index]\n",
        "        if str(row[col_index]) == val:  # Ensure value matches before updating\n",
        "            old_value = row[setcol_index]  # Capture old value for index update if needed\n",
        "            row[setcol_index] = setval\n",
        "            updated_rows += 1\n",
        "            # Update index if the modified column is indexed\n",
        "            if setcol in dynamic_db.indices[table_name]:\n",
        "                dynamic_db.indices[table_name][setcol].pop(old_value, None)  # Remove old index entry\n",
        "                if setval not in dynamic_db.indices[table_name][setcol]:\n",
        "                    dynamic_db.indices[table_name][setcol][setval] = []\n",
        "                dynamic_db.indices[table_name][setcol][setval].append(index)\n",
        "\n",
        "    # Feedback to the user about the operation\n",
        "    print(f\"Updated {updated_rows} row(s) where {col} = {val} setting {setcol} to {setval}.\")\n",
        "\n",
        "    # Optionally, debug print to confirm data change\n",
        "    print(f\"First few records post-update: {data[:4]}\")  # Print first few records to confirm\n",
        "\n",
        "\n",
        "def update_data(database, dynamic_db, table_name, setcol, setval, col, val):\n",
        "    if table_name not in database:\n",
        "        print(f\"Table '{table_name}' not found.\")\n",
        "        return\n",
        "\n",
        "    dynamic_db.check_index(table_name, col)\n",
        "    matched_indices = dynamic_db.query_by_index(table_name, col, val)\n",
        "    if not matched_indices:\n",
        "        print(f\"No records found where {col} = {val}.\")\n",
        "        return\n",
        "\n",
        "    table = database[table_name]\n",
        "    headers = table['headers']\n",
        "    data = table['data']\n",
        "\n",
        "    setcol_index = headers.index(setcol)\n",
        "    col_index = headers.index(col)\n",
        "\n",
        "    updated_rows = 0\n",
        "    for index in matched_indices:\n",
        "        row = data[index]\n",
        "        if str(row[col_index]) == val:  # Check matching condition\n",
        "            # Convert types for consistency in indices\n",
        "            old_value = str(row[setcol_index])\n",
        "            new_value = str(setval)\n",
        "\n",
        "            row[setcol_index] = new_value\n",
        "            updated_rows += 1\n",
        "\n",
        "            # Update index if the modified column is indexed\n",
        "            if setcol in dynamic_db.indices[table_name]:\n",
        "                # Remove old value entry from index\n",
        "                if old_value in dynamic_db.indices[table_name][setcol]:\n",
        "                    dynamic_db.indices[table_name][setcol][old_value].remove(index)\n",
        "                    if not dynamic_db.indices[table_name][setcol][old_value]:  # Clean up if empty\n",
        "                        del dynamic_db.indices[table_name][setcol][old_value]\n",
        "\n",
        "                # Add new value entry to index\n",
        "                if new_value not in dynamic_db.indices[table_name][setcol]:\n",
        "                    dynamic_db.indices[table_name][setcol][new_value] = []\n",
        "                dynamic_db.indices[table_name][setcol][new_value].append(index)\n",
        "\n",
        "    print(f\"Updated {updated_rows} row(s) where {col} = {val} setting {setcol} to {setval}.\")"
      ],
      "metadata": {
        "id": "f48Cyv54as4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONG2Xl6zsfJA"
      },
      "outputs": [],
      "source": [
        "# Update table query parse implementation\n",
        "def update(command, pos):\n",
        "  tables = [] # Stores table names\n",
        "  setvalue = [] # Stores values that have to be updated\n",
        "  values = [] # Stores condition of the values that are to be updated\n",
        "  cmd = str(command).split()\n",
        "  if pos['update'] == 0 and pos['set'] == 2 and pos['where']:\n",
        "    tables = cmd[1]\n",
        "    setvalue = cmd[3:pos['where']]\n",
        "    setvalue = clean(setvalue)\n",
        "    values = cmd[pos['where']+1:len(cmd)]\n",
        "    values = clean(values)\n",
        "    c = setvalue.index('=')\n",
        "    setcol, setval = setvalue[0:c], setvalue[c+1:len(setvalue)]\n",
        "    setcol = setcol[0]\n",
        "    setval = setval[0]\n",
        "    c = values.index('=')\n",
        "    col, val = values[0:c], values[c+1:len(values)]\n",
        "    col = col[0]\n",
        "    val = val[0]\n",
        "    if tables not in db.keys(): # Checking if the table exists\n",
        "      print('Table not found')\n",
        "    elif setcol not in db[tables]: # Checking if the columns exists\n",
        "      print('Column not found')\n",
        "    elif col not in db[tables]: # Checking if the columns exists\n",
        "      print('Column not found')\n",
        "    else: # Updating data\n",
        "      # *************** Update table function implementation - use tables(list), setvalue(dict) and values(dict) as parameters\n",
        "      update_data(databases, dynamic_db, tables, setcol, setval, col, val)\n",
        "      pass\n",
        "  else:\n",
        "    print('Invalid Query')\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlPMPRUUgDoC"
      },
      "source": [
        "# Delete Record Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiSEm0Vzgweg"
      },
      "outputs": [],
      "source": [
        "def delete_data(database, table_name, column_name, value):\n",
        "    if table_name not in database:\n",
        "        print(\"Table not found\")\n",
        "        return\n",
        "\n",
        "    # Accessing the table\n",
        "    table = database[table_name]\n",
        "    headers = table['headers']\n",
        "\n",
        "    if column_name not in headers:\n",
        "        print(\"Column not found\")\n",
        "        return\n",
        "\n",
        "    # Finding the index of the column to check\n",
        "    column_index = headers.index(column_name)\n",
        "\n",
        "    # Filtering out rows that match the deletion criterion\n",
        "    original_row_count = len(table['data'])\n",
        "    table['data'] = [row for row in table['data'] if str(row[column_index]) != str(value)]\n",
        "\n",
        "    if len(table['data']) < original_row_count:\n",
        "        print(f\"Deleted {original_row_count - len(table['data'])} records where {column_name} is {value}\")\n",
        "    else:\n",
        "        print(\"No records found to delete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F54jbuuA1P0t"
      },
      "source": [
        "# Delete parser code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "934sP-vC1RoE"
      },
      "outputs": [],
      "source": [
        "def delete(command, pos):\n",
        "  tables = [] # Stores table names\n",
        "  delvalue = [] # Stores values to be deleted\n",
        "  cmd = str(command).split()\n",
        "  if pos['delete'] == 0 and pos['from'] == 1 and pos['where'] == 3:\n",
        "    tables = cmd[2]\n",
        "    delvalue = cmd[4:len(cmd)]\n",
        "    delvalue = clean(delvalue)\n",
        "    c = delvalue.index('=')\n",
        "    col, val = delvalue[0:c], delvalue[c+1:len(delvalue)]\n",
        "    col = col[0]\n",
        "    val = val[0]\n",
        "    if tables not in db.keys(): # Checking if the table exists\n",
        "      print('Table not found')\n",
        "    elif col not in db[tables]: # Checking if the columns exists\n",
        "      print('Column not found')\n",
        "    else: # Deleting data\n",
        "      # *************** Delete table function implementation - use tables(list), col and val as parameters\n",
        "      delete_data(databases, tables, col, val)\n",
        "  else:\n",
        "    print('Invalid Query')\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q8yyKv_jpFe"
      },
      "source": [
        "# Dropping a Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EqOqsFEjqdF"
      },
      "outputs": [],
      "source": [
        "def drop_table(db, table_name):\n",
        "    if table_name in db:\n",
        "        del db[table_name]\n",
        "        print(\"Table '{table_name}' has been dropped successfully.\")\n",
        "    else:\n",
        "        return f\"Error: Table '{table_name}' does not exist in the database.\"\n",
        "\n",
        "    # List current tables in the database\n",
        "    if databases:\n",
        "        current_tables = ', '.join(db.keys())\n",
        "        # print(f\"Current tables in database: {current_tables}\")\n",
        "    else:\n",
        "        print(\"No tables left in the database.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXG1n8zyvWnt"
      },
      "source": [
        "# Dropping a table parser function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmi0exf_vV7T"
      },
      "outputs": [],
      "source": [
        "def drop(command, databases):\n",
        "    cmd = command.split()\n",
        "    if cmd[0].lower() == 'drop' and cmd[1].lower() == 'table':\n",
        "        table_name = cmd[2]\n",
        "        table_name = table_name.replace(';', '')  # Remove any trailing semicolons\n",
        "        # print(f\"Attempting to drop table: {table_name}\")\n",
        "        print(databases)\n",
        "        if table_name not in db.keys():\n",
        "          print('Table not found')\n",
        "        else:\n",
        "          del db[table_name]\n",
        "          drop_table(databases, table_name)\n",
        "    else:\n",
        "        print('Invalid Query')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiyPJV2yk3Gt"
      },
      "source": [
        "Select * function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIzv2bMDk46F"
      },
      "outputs": [],
      "source": [
        "def display_all_tables(databases):\n",
        "    for db_name, contents in databases.items():\n",
        "        print(f\"\\n{db_name}:\")\n",
        "        print(tabulate(contents['data'], headers=contents['headers'], tablefmt=\"grid\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i28B4jLvwnFo"
      },
      "source": [
        "Select all parser implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7M8yfGswptE"
      },
      "outputs": [],
      "source": [
        "def show_tables(command, databases):\n",
        "    print(str(command).strip().lower())\n",
        "    if str(command).strip().lower() == 'show tables;':\n",
        "        display_all_tables(db)\n",
        "    else:\n",
        "        print('Invalid Query')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-LOTbVXlV6J"
      },
      "source": [
        "Order by function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE-AxOPwlXUW"
      },
      "outputs": [],
      "source": [
        "def order_by(database, table_name, column_name, ascending=True):\n",
        "    if table_name not in database:\n",
        "        raise ValueError(f\"Table '{table_name}' not found.\")\n",
        "\n",
        "    table_dict = database[table_name]\n",
        "    headers = table_dict['headers']\n",
        "    data = table_dict['data']\n",
        "\n",
        "    if column_name not in headers:\n",
        "        print(f\"Column '{column_name}' not found in the dataset.\")\n",
        "\n",
        "    column_index = headers.index(column_name)\n",
        "    # Sort the data based on the specified column\n",
        "    sorted_data = sorted(data, key=lambda x: x[column_index] if x[column_index] is not None else float('-inf') if ascending else float('inf'), reverse=not ascending)\n",
        "\n",
        "    # Print the sorted data using tabulate\n",
        "    print(tabulate(sorted_data, headers=headers, tablefmt=\"grid\"))\n",
        "\n",
        "    return sorted_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNtlk9YfnHyK"
      },
      "source": [
        "Average Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnMdViUMnJZq"
      },
      "outputs": [],
      "source": [
        "def col_average(database, table_name, column_name):\n",
        "    if table_name not in database:\n",
        "        print(f\"Table '{table_name}' not found.\")\n",
        "\n",
        "    table_dict = database[table_name]\n",
        "    headers = table_dict['headers']\n",
        "    data = table_dict['data']\n",
        "\n",
        "    if column_name not in headers:\n",
        "        print(f\"Column '{column_name}' not found in the dataset.\")\n",
        "\n",
        "    column_index = headers.index(column_name)\n",
        "\n",
        "    # Collect all non-null numeric entries for the column\n",
        "    valid_entries = [float(row[column_index]) for row in data if row[column_index] is not None and is_number(row[column_index])]\n",
        "\n",
        "    if not valid_entries:\n",
        "        print(f\"No valid numeric data found in column '{column_name}'.\")\n",
        "\n",
        "    # Calculate the average of these entries\n",
        "    return sum(valid_entries) / len(valid_entries)\n",
        "\n",
        "def is_number(value):\n",
        "    \"\"\" Helper function to determine if a value is numeric. \"\"\"\n",
        "    try:\n",
        "        float(value)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOjSSbyvoPtU"
      },
      "source": [
        "Count function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E-BQEXcoRGp"
      },
      "outputs": [],
      "source": [
        "def col_count(database, table_name, column_name):\n",
        "    if table_name not in database:\n",
        "        print(f\"Table '{table_name}' not found.\")\n",
        "\n",
        "    table_dict = database[table_name]\n",
        "    headers = table_dict['headers']\n",
        "    data = table_dict['data']\n",
        "\n",
        "    if column_name not in headers:\n",
        "        print(f\"Column '{column_name}' not found in the dataset.\")\n",
        "\n",
        "    column_index = headers.index(column_name)\n",
        "    # Count non-null entries in the specified column\n",
        "    return len([row[column_index] for row in data if row[column_index] is not None])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jGKqYYBodUu"
      },
      "source": [
        "Min and Max Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYu_MKQHoa9V"
      },
      "outputs": [],
      "source": [
        "def col_min(database, table_name, column_name):\n",
        "    if table_name not in database:\n",
        "        print(f\"Table '{table_name}' not found.\")\n",
        "\n",
        "    table_dict = database[table_name]\n",
        "    headers = table_dict['headers']\n",
        "    data = table_dict['data']\n",
        "\n",
        "    if column_name not in headers:\n",
        "        print(f\"Column '{column_name}' not found in the dataset.\")\n",
        "\n",
        "    column_index = headers.index(column_name)\n",
        "\n",
        "    # Retrieve all numeric entries and find the minimum\n",
        "    try:\n",
        "        return min(float(row[column_index]) for row in data if row[column_index] is not None and is_number(row[column_index]))\n",
        "    except ValueError:\n",
        "        print(f\"No valid numeric data found in column '{column_name}' for minimum calculation.\")\n",
        "\n",
        "def col_max(database, table_name, column_name):\n",
        "    if table_name not in database:\n",
        "        print(f\"Table '{table_name}' not found.\")\n",
        "\n",
        "    table_dict = database[table_name]\n",
        "    headers = table_dict['headers']\n",
        "    data = table_dict['data']\n",
        "\n",
        "    if column_name not in headers:\n",
        "       print(f\"Column '{column_name}' not found in the dataset.\")\n",
        "\n",
        "    column_index = headers.index(column_name)\n",
        "\n",
        "    # Retrieve all numeric entries and find the maximum\n",
        "    try:\n",
        "        return max(float(row[column_index]) for row in data if row[column_index] is not None and is_number(row[column_index]))\n",
        "    except ValueError:\n",
        "        print(f\"No valid numeric data found in column '{column_name}' for maximum calculation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OLGKgd8o7R1"
      },
      "source": [
        "Sum Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z6A_sgqo8jp"
      },
      "outputs": [],
      "source": [
        "def col_sum(database, table_name, column_name):\n",
        "    if table_name not in database:\n",
        "        print(f\"Table '{table_name}' not found.\")\n",
        "\n",
        "    table_dict = database[table_name]\n",
        "    headers = table_dict['headers']\n",
        "    data = table_dict['data']\n",
        "\n",
        "    if column_name not in headers:\n",
        "        print(f\"Column '{column_name}' not found in the dataset.\")\n",
        "\n",
        "    column_index = headers.index(column_name)\n",
        "\n",
        "    # Sum all numeric entries in the specified column\n",
        "    try:\n",
        "        total = sum(float(row[column_index]) for row in data if row[column_index] is not None and is_number(row[column_index]))\n",
        "        return total\n",
        "    except ValueError:\n",
        "        print(f\"No valid numeric data found in column '{column_name}' for sum calculation.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMQE9TVjqDQd"
      },
      "source": [
        "# Optimised Join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "FFreVMijh7wl",
        "outputId": "7d39c389-f473-453a-9a3d-5d4480ea799c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def create_table(rows, pattern=1):\\n  # Pattern - 1 : [[1, 1], [2, 2], ... ]\\n  # Pattern - 2 : [[1, 1], [2, 1], ... ]\\n  return [[i, i if pattern == 1 else 1] for i in range(1, rows + 1)]\\n\\n# Creating tables with headers and data\\nd1 = {\\'headers\\': [\\'col1\\', \\'col2\\'], \\'data\\': create_table(1000)}\\nd2 = {\\'headers\\': [\\'col1\\', \\'col2\\'], \\'data\\': create_table(1000, pattern=2)}\\nd3 = {\\'headers\\': [\\'col1\\', \\'col2\\'], \\'data\\': create_table(10000, pattern=1)}\\nd4 = {\\'headers\\': [\\'col1\\', \\'col2\\'], \\'data\\': create_table(10000, pattern=2)}\\n\\n# Print sample data to verify\\nprint(\"Data_1\", d1)\\nprint(\"Data_2\", d2)\\nprint(\"Data_3\", d3)\\nprint(\"Data_4\", d4)\\n\\n# Print length of sample data\\nprint(len(d1[\\'data\\']))\\nprint(len(d2[\\'data\\']))\\nprint(len(d3[\\'data\\']))\\nprint(len(d4[\\'data\\']))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "'''def create_table(rows, pattern=1):\n",
        "  # Pattern - 1 : [[1, 1], [2, 2], ... ]\n",
        "  # Pattern - 2 : [[1, 1], [2, 1], ... ]\n",
        "  return [[i, i if pattern == 1 else 1] for i in range(1, rows + 1)]\n",
        "\n",
        "# Creating tables with headers and data\n",
        "d1 = {'headers': ['col1', 'col2'], 'data': create_table(1000)}\n",
        "d2 = {'headers': ['col1', 'col2'], 'data': create_table(1000, pattern=2)}\n",
        "d3 = {'headers': ['col1', 'col2'], 'data': create_table(10000, pattern=1)}\n",
        "d4 = {'headers': ['col1', 'col2'], 'data': create_table(10000, pattern=2)}\n",
        "\n",
        "# Print sample data to verify\n",
        "print(\"Data_1\", d1)\n",
        "print(\"Data_2\", d2)\n",
        "print(\"Data_3\", d3)\n",
        "print(\"Data_4\", d4)\n",
        "\n",
        "# Print length of sample data\n",
        "print(len(d1['data']))\n",
        "print(len(d2['data']))\n",
        "print(len(d3['data']))\n",
        "print(len(d4['data']))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rUKo5jBiqEkL",
        "outputId": "3aaad901-5fa3-4de8-e4c7-8cf4548aeaf5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Usage of the join function\\njoined_data, headers = optimised_join(d3, d4, 'col1', 'col1')\\nprint(tabulate(joined_data, headers=headers, tablefmt='grid'))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "'''def basic_join(table1, table2, join_on1, join_on2):\n",
        "    headers1 = table1['headers']\n",
        "    headers2 = table2['headers']\n",
        "    data1 = table1['data']\n",
        "    data2 = table2['data']\n",
        "\n",
        "    # Finding the index of the join columns\n",
        "    join_index1 = headers1.index(join_on1)\n",
        "    join_index2 = headers2.index(join_on2)\n",
        "\n",
        "    # Performing the join\n",
        "    joined_data = []\n",
        "    for row1 in data1:\n",
        "        for row2 in data2:\n",
        "            if row1[join_index1] == row2[join_index2]:\n",
        "                # Create a new row combining both rows, excluding the join column from the second table\n",
        "                new_row = row1 + [item for idx, item in enumerate(row2) if idx != join_index2]\n",
        "                joined_data.append(new_row)\n",
        "\n",
        "    return joined_data\n",
        "\n",
        "# eg usage\n",
        "joined_data = basic_join(databases['Data_1'], databases['Data_3'], 'col1', 'col1')\n",
        "\n",
        "# Display the joined data\n",
        "for row in joined_data:\n",
        "    print(row)'''\n",
        "\n",
        "def optimised_join(table1, table2, join_on1, join_on2, numeric=False):\n",
        "    # Determine if tables should be sorted or swapped\n",
        "    if len(table1['data']) > len(table2['data']):\n",
        "        table1, table2 = table2, table1\n",
        "        join_on1, join_on2 = join_on2, join_on1\n",
        "\n",
        "    # Get indices of the join columns from the headers\n",
        "    join_index1 = table1['headers'].index(join_on1)\n",
        "    join_index2 = table2['headers'].index(join_on2)\n",
        "\n",
        "    # Create a lookup dictionary for the first table\n",
        "    lookup = {}\n",
        "    for row in table1['data']:\n",
        "        key = int(row[join_index1]) if numeric else row[join_index1]\n",
        "        if key not in lookup:\n",
        "            lookup[key] = []\n",
        "        lookup[key].append(row)\n",
        "\n",
        "    # Perform the join using the lookup\n",
        "    joined_data = []\n",
        "    for row2 in table2['data']:\n",
        "        key = int(row2[join_index2]) if numeric else row2[join_index2]\n",
        "        if key in lookup:\n",
        "            for row1 in lookup[key]:\n",
        "                # Create a new row combining both rows, excluding the duplicate join key\n",
        "                new_row = row1[:] + [item for idx, item in enumerate(row2) if idx != join_index2]\n",
        "                joined_data.append(new_row)\n",
        "\n",
        "    return joined_data, table1['headers'] + [col for col in table2['headers'] if col != join_on2]\n",
        "\n",
        "# Example usage:\n",
        "table1 = {'headers': ['id', 'name'], 'data': [['1', 'Alice'], ['2', 'Bob']]}\n",
        "table2 = {'headers': ['id', 'age'], 'data': [['1', '24'], ['2', '30']]}\n",
        "joined_data, headers = optimised_join(table1, table2, 'id', 'id', numeric=True)\n",
        "\n",
        "\"\"\"\n",
        "# Usage of the join function\n",
        "joined_data, headers = optimised_join(d3, d4, 'col1', 'col1')\n",
        "print(tabulate(joined_data, headers=headers, tablefmt='grid'))\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiYXD2zEqjKq"
      },
      "source": [
        "# Saving data back to csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIw6EtBeqllJ",
        "outputId": "a23ac038-2688-4cbf-ff6b-e5980e8c579c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for 'data_1' saved to 'relation1.csv'.\n",
            "Data for 'data_2' saved to 'relation2.csv'.\n",
            "Data for 'data_3' saved to 'relation3.csv'.\n",
            "Data for 'data_4' saved to 'relation4.csv'.\n",
            "Data for 'visitors' saved to 'visitors.csv'.\n",
            "Data for 'rides' saved to 'rides.csv'.\n"
          ]
        }
      ],
      "source": [
        "def save_databases_to_csv(databases, file_paths):\n",
        "    for db_name, contents in databases.items():\n",
        "        if db_name in file_paths:\n",
        "            file_path = file_paths[db_name]\n",
        "            with open(file_path, 'w', newline='') as csvfile:\n",
        "                writer = csv.writer(csvfile)\n",
        "                writer.writerow(contents['headers'])  # write headers\n",
        "                writer.writerows(contents['data'])    # write data rows\n",
        "            print(f\"Data for '{db_name}' saved to '{file_path}'.\")\n",
        "        else:\n",
        "            print(f\"No file path provided for '{db_name}'. Cannot save to CSV.\")\n",
        "\n",
        "save_databases_to_csv(databases, file_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF0TucCSsEBT"
      },
      "source": [
        "# Create Table Function and parsing code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "qJt8BxhGsHnB",
        "outputId": "8398dabd-bf4d-4e8f-f0ef-47638053838d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncommand = \"create table Data_1 if not exists (ID, Name, Position);\"\\ncreate_table_from_command(command, databases)\\n\\n# Output the database to verify the table was created\\nprint(databases) '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "def create_table(table_name, columns):\n",
        "  # Create the table if it doesn't exist\n",
        "  databases[table_name] = {'headers': columns, 'data': []}\n",
        "  print(f\"Table '{table_name}' created successfully with columns {columns}\")\n",
        "  file_paths[table_name] = str(table_name)+'.csv'\n",
        "\n",
        "\"\"\"\n",
        "command = \"create table Data_1 if not exists (ID, Name, Position);\"\n",
        "create_table_from_command(command, databases)\n",
        "\n",
        "# Output the database to verify the table was created\n",
        "print(databases) \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhSCtpnO0LXB"
      },
      "source": [
        "# Create Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aoherdh0L4Q"
      },
      "outputs": [],
      "source": [
        "# Create table query parse implementation\n",
        "def create(command, pos):\n",
        "  tables = [] # Stores table names\n",
        "  values = [] # Stores values of the table\n",
        "  d = False # Duplicate value\n",
        "  cmd = str(command).split()\n",
        "  if pos['create'] == 0 and pos['table'] == 1 and pos['if'] == 2 and pos['not'] == 3 and pos['exists'] == 4:\n",
        "    tables = cmd[5]\n",
        "    values = cmd[6:len(cmd)]\n",
        "    values = clean(values)\n",
        "    if 'primary' in values and 'key' in values:\n",
        "      values.remove('primary')\n",
        "      values.remove('key')\n",
        "    if 'foreign' in values and 'key' in values:\n",
        "      values.remove('foreign')\n",
        "      values.remove('key')\n",
        "    values = format(values)\n",
        "    d = duplicates(values)\n",
        "    if d == True:\n",
        "      print('Duplicate values')\n",
        "      return None\n",
        "    if tables in db.keys(): # Checking if the table exists\n",
        "      print('Table already exists')\n",
        "      return None\n",
        "    else: # Creating table\n",
        "      # *************** Create table function implementation - use tables(list) and values(dict) as parameters\n",
        "      db[tables] = values.keys() # DON'T REMOVE THIS\n",
        "      create_table(tables, values.keys())\n",
        "  else:\n",
        "    print('Invalid Query')\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWShwam-0MSA"
      },
      "source": [
        "# Insert Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MgE0CoO0Nwf"
      },
      "outputs": [],
      "source": [
        "def insert_table(table_name, columns):\n",
        "    # Check if the table exists in the databases dictionary\n",
        "    if table_name in databases:\n",
        "        # Check if the number of elements in columns matches the headers\n",
        "        if len(columns) == len(databases[table_name]['headers']):\n",
        "            # Append the new row to the table's data\n",
        "            databases[table_name]['data'].append(columns)\n",
        "        else:\n",
        "            print(\"The number of provided columns does not match the table's headers.\")\n",
        "    else:\n",
        "        print(\"Table does not exist in the database.\")\n",
        "    return None\n",
        "\n",
        "# Insert table query parse implementation\n",
        "def insert(command, pos):\n",
        "  tables = [] # Stores table names\n",
        "  values = [] # Stores values that are to be inserted\n",
        "  cmd = str(command).split()\n",
        "  if pos['insert'] == 0 and pos['into'] == 1 and pos['values']:\n",
        "    tables = cmd[2]\n",
        "    columns = cmd[3:pos['values']]\n",
        "    columns = clean(columns)\n",
        "    values = cmd[pos['values']+1:len(cmd)]\n",
        "    values = clean(values)\n",
        "    if tables not in db.keys(): # Checking if the table exists\n",
        "      print('Table not found')\n",
        "    elif len(values) != len(db[tables]):\n",
        "      print('Missing fields')\n",
        "    else: # Inserting data\n",
        "      # *************** Insert table function implementation - use tables(list) and values(list) as parameters\n",
        "      # print('Calling insert into table function')\n",
        "      data = databases[tables]['data']\n",
        "      headers = databases[tables]['headers']\n",
        "      pk_col_name = keys[tables]['pk']\n",
        "      pk_index = headers.index(pk_col_name)\n",
        "      # Check for duplicate primary key\n",
        "      if data != []:\n",
        "          for row in data:\n",
        "            if row[pk_index] == values[pk_index]:\n",
        "              print('Duplicate key')\n",
        "              return None\n",
        "      insert_table(tables, values)\n",
        "  else:\n",
        "    print('Invalid Query')\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Select"
      ],
      "metadata": {
        "id": "v7RlegYKLmO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simple select\n",
        "def simple_select(database, table_name, cols, limit=None):\n",
        "    if table_name not in database:\n",
        "        print(f\"Table '{table_name}' not found.\")\n",
        "\n",
        "    table_dict = database[table_name]\n",
        "    headers = table_dict['headers']\n",
        "    data = table_dict['data']\n",
        "\n",
        "    # Determine which columns to display\n",
        "    if \"*\" in cols:\n",
        "        selected_columns = headers  # All columns\n",
        "        column_indices = list(range(len(headers)))\n",
        "    else:\n",
        "        selected_columns = [col for col in cols if col in headers]\n",
        "        column_indices = [headers.index(col) for col in selected_columns]\n",
        "\n",
        "    # Filter and reshape the data based on selected columns\n",
        "    result_data = [[row[idx] for idx in column_indices] for row in data]\n",
        "\n",
        "    # Apply the limit if there is one\n",
        "    if limit is not None:\n",
        "        result_data = result_data[:int(limit)]\n",
        "\n",
        "    # Display the result in a formatted table using tabulate\n",
        "    print(tabulate(result_data, headers=selected_columns, tablefmt=\"grid\"))\n",
        "\n",
        "    return result_data"
      ],
      "metadata": {
        "id": "G7kb6uXALphi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# And or"
      ],
      "metadata": {
        "id": "HoYpSzjabbG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_with_and_or(database, table_name, cols, col1, op1, val1, col2, op2, val2, clause):\n",
        "    # Check if the table exists\n",
        "    if table_name not in database:\n",
        "        print(f\"Table '{table_name}' not found.\")\n",
        "        return\n",
        "\n",
        "    # Retrieve table data\n",
        "    table = database[table_name]\n",
        "    headers = table['headers']\n",
        "    data = table['data']\n",
        "\n",
        "    # Check if columns exist in headers\n",
        "    if not (col1 in headers and col2 in headers):\n",
        "        missing_cols = [col for col in [col1, col2] if col not in headers]\n",
        "        print(f\"Column(s) {missing_cols} not found in the dataset.\")\n",
        "        return\n",
        "\n",
        "    # Determine indices for the filter columns\n",
        "    col1_index = headers.index(col1)\n",
        "    col2_index = headers.index(col2)\n",
        "\n",
        "    # Filter function based on operators\n",
        "    def check_condition(value, operator, comparison_value):\n",
        "        if operator == '=':\n",
        "            return value == comparison_value\n",
        "        elif operator == '>':\n",
        "            return value > comparison_value\n",
        "        elif operator == '<':\n",
        "            return value < comparison_value\n",
        "        elif operator == '>=':\n",
        "            return value >= comparison_value\n",
        "        elif operator == '<=':\n",
        "            return value <= comparison_value\n",
        "        elif operator == '!=':\n",
        "            return value != comparison_value\n",
        "        return False\n",
        "\n",
        "    # Apply filters based on the clause\n",
        "    if clause.lower() == 'and':\n",
        "        filtered_data = [row for row in data if check_condition(row[col1_index], op1, val1) and check_condition(row[col2_index], op2, val2)]\n",
        "    elif clause.lower() == 'or':\n",
        "        filtered_data = [row for row in data if check_condition(row[col1_index], op1, val1) or check_condition(row[col2_index], op2, val2)]\n",
        "    else:\n",
        "        print(\"Invalid logical clause specified. Use 'AND' or 'OR'.\")\n",
        "        return\n",
        "\n",
        "    # Select columns to display\n",
        "    if '*' in cols:\n",
        "        selected_columns = headers\n",
        "        display_data = filtered_data\n",
        "    else:\n",
        "        if not set(cols).issubset(headers):\n",
        "            print(f\"Some specified columns {cols} are not found in the dataset.\")\n",
        "            return\n",
        "        selected_columns = [col for col in cols if col in headers]\n",
        "        display_data = [[row[headers.index(col)] for col in selected_columns] for row in filtered_data]\n",
        "\n",
        "    # Display the results\n",
        "    print(tabulate(display_data, headers=selected_columns, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "id": "vWgUUxkzbaFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Order by"
      ],
      "metadata": {
        "id": "QDk7FAAOe1Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def order_by(database, table_name, cols, column_name, ord=\"asc\"):\n",
        "    if table_name not in database:\n",
        "        print(f\"Table '{table_name}' not found.\")\n",
        "\n",
        "    table_dict = database[table_name]\n",
        "    headers = table_dict['headers']\n",
        "    data = table_dict['data']\n",
        "\n",
        "    if column_name not in headers:\n",
        "        print(f\"Column '{column_name}' not found in the dataset.\")\n",
        "\n",
        "    # Determine which columns to display\n",
        "    if \"*\" in cols:\n",
        "        selected_columns = headers  # Select all columns if '*' is specified\n",
        "        column_indices = range(len(headers))  # Use all indices\n",
        "    else:\n",
        "        selected_columns = [col for col in cols if col in headers]\n",
        "        if not selected_columns:\n",
        "            print(\"None of the specified columns are found in the dataset.\")\n",
        "        column_indices = [headers.index(col) for col in selected_columns]\n",
        "\n",
        "    # Get the index of the column to sort by\n",
        "    sort_index = headers.index(column_name)\n",
        "\n",
        "    # Determine if the column data should be treated as numbers\n",
        "    all_numbers = all(isinstance(row[sort_index], (int, float)) or (isinstance(row[sort_index], str) and row[sort_index].isdigit()) for row in data if row[sort_index] is not None)\n",
        "\n",
        "    # Sort the data based on the specified column, converting to float if they're all numbers\n",
        "    sorted_data = sorted(\n",
        "        data,\n",
        "        key=lambda x: (float('inf') if x[sort_index] is None else float(x[sort_index]) if all_numbers else x[sort_index]),\n",
        "        reverse=ord != \"asc\"\n",
        "    )\n",
        "\n",
        "    # Filter and reshape the data based on selected columns after sorting\n",
        "    sorted_filtered_data = [[row[idx] for idx in column_indices] for row in sorted_data]\n",
        "\n",
        "    # Print the sorted and filtered data using tabulate\n",
        "    print(tabulate(sorted_filtered_data, headers=selected_columns, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "id": "PbMq8koHe2z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group by"
      ],
      "metadata": {
        "id": "j4LbWP_OfVkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_group_by(database, table_name, cols, group_by_column):\n",
        "    if table_name not in database:\n",
        "        print(f\"Table '{table_name}' not found.\")\n",
        "\n",
        "    table_dict = database[table_name]\n",
        "    headers = table_dict['headers']\n",
        "    data = table_dict['data']\n",
        "\n",
        "    # Check if the group_by column is in the headers\n",
        "    if group_by_column not in headers:\n",
        "        print(f\"Group by column '{group_by_column}' not found in the dataset.\")\n",
        "\n",
        "    group_by_index = headers.index(group_by_column)\n",
        "\n",
        "    # Determine which columns to display\n",
        "    if \"*\" in cols:\n",
        "        selected_columns = headers  # Select all columns\n",
        "        column_indices = range(len(headers))  # Use all indices\n",
        "    else:\n",
        "        selected_columns = [col for col in cols if col in headers]\n",
        "        if not selected_columns:\n",
        "            print(\"None of the specified columns are found in the dataset.\")\n",
        "        column_indices = [headers.index(col) for col in selected_columns]\n",
        "\n",
        "    # Create a dictionary to hold grouped data\n",
        "    grouped_data = defaultdict(list)\n",
        "\n",
        "    # Populate the grouped data\n",
        "    for row in data:\n",
        "        group_key = row[group_by_index]\n",
        "        selected_values = [row[idx] for idx in column_indices]\n",
        "        grouped_data[group_key].append(selected_values)\n",
        "\n",
        "    # Display the result\n",
        "    for key, group in grouped_data.items():\n",
        "        print(f\"Group: {key}\")\n",
        "        print(tabulate(group, headers=selected_columns, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "id": "Kf2PtVClfYjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple where clause"
      ],
      "metadata": {
        "id": "jdgk5Nqqm_kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_where(database, table_name, cols, col1, op1, val1):\n",
        "    # Check if the table exists\n",
        "    if table_name not in database:\n",
        "        print(f\"Table '{table_name}' not found.\")\n",
        "        return\n",
        "\n",
        "    # Retrieve table data\n",
        "    table = database[table_name]\n",
        "    headers = table['headers']\n",
        "    data = table['data']\n",
        "\n",
        "    # Check if columns exist in headers\n",
        "    if not (col1 in headers):\n",
        "        missing_cols = [col for col in [col1] if col not in headers]\n",
        "        print(f\"Column(s) {missing_cols} not found in the dataset.\")\n",
        "        return\n",
        "\n",
        "    # Determine indices for the filter columns\n",
        "    col1_index = headers.index(col1)\n",
        "\n",
        "    # Filter function based on operators\n",
        "    def check_condition(value, operator, comparison_value):\n",
        "        if operator == '=':\n",
        "            return value == comparison_value\n",
        "        elif operator == '>':\n",
        "            return value > comparison_value\n",
        "        elif operator == '<':\n",
        "            return value < comparison_value\n",
        "        elif operator == '>=':\n",
        "            return value >= comparison_value\n",
        "        elif operator == '<=':\n",
        "            return value <= comparison_value\n",
        "        elif operator == '!=':\n",
        "            return value != comparison_value\n",
        "        return False\n",
        "\n",
        "    filtered_data = [row for row in data if check_condition(row[col1_index], op1, val1)]\n",
        "\n",
        "    # Select columns to display\n",
        "    if '*' in cols:\n",
        "        selected_columns = headers\n",
        "        display_data = filtered_data\n",
        "    else:\n",
        "        if not set(cols).issubset(headers):\n",
        "            print(f\"Some specified columns {cols} are not found in the dataset.\")\n",
        "            return\n",
        "        selected_columns = [col for col in cols if col in headers]\n",
        "        display_data = [[row[headers.index(col)] for col in selected_columns] for row in filtered_data]\n",
        "\n",
        "    # Display the results\n",
        "    print(tabulate(display_data, headers=selected_columns, tablefmt=\"grid\"))"
      ],
      "metadata": {
        "id": "YFZoxhNFnCPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JAZ63aS0OCq"
      },
      "source": [
        "# Select Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMWnXBw_21-C"
      },
      "outputs": [],
      "source": [
        "def columns(select_clause):\n",
        "    select = select_clause.replace(',', ' ').split()\n",
        "    cols = []\n",
        "    aggr = None\n",
        "    for part in select:\n",
        "        if '(' in part and ')' in part:\n",
        "            func_start = part.find('(')\n",
        "            func_end = part.find(')')\n",
        "            if func_start != -1 and func_end != -1 and func_start < func_end:\n",
        "                func_name = part[:func_start].strip().lower()\n",
        "                column_name = part[func_start+1:func_end].strip()\n",
        "                aggr = (func_name, column_name)\n",
        "        else:\n",
        "            cols.append(part.strip())\n",
        "    if aggr:\n",
        "        return aggr[1], aggr[0]\n",
        "    elif cols:\n",
        "        return cols , None\n",
        "\n",
        "def from_condition(from_clause):\n",
        "    columns = str(from_clause).split(' ')\n",
        "    table = columns[0]\n",
        "    if 'limit' in from_clause:\n",
        "      limit = columns[2]\n",
        "      return table, limit\n",
        "    else:\n",
        "      return table, None\n",
        "\n",
        "def where_condition(where_clause):\n",
        "    where = str(where_clause).split(' ')\n",
        "    clause = ''\n",
        "    fnot = False\n",
        "    wclause = False\n",
        "    if 'not in' in where_clause:\n",
        "      col = where[0]\n",
        "      val = str(where[3:len(where)]).replace('(','').replace(')','').replace(\"'\",'').replace('\"','').replace(',','').replace('[','').replace(']','').split(' ')\n",
        "      fnot = True\n",
        "      wclause = True\n",
        "      clause = 'not in'\n",
        "    elif 'in' in where_clause and fnot == False:\n",
        "      col = where[0]\n",
        "      val = str(where[2:len(where)]).replace('(','').replace(')','').replace(\"'\",'').replace('\"','').replace(',','').replace('[','').replace(']','').split(' ')\n",
        "      wclause = True\n",
        "      clause = 'in'\n",
        "    elif 'like' in where_clause:\n",
        "      col = where[0]\n",
        "      val = str(where[2:len(where)]).replace('(','').replace(')','').replace(\"'\",'').replace('\"','').replace(',','').replace('[','').replace(']','').split(' ')\n",
        "      val = val[0]\n",
        "      wclause = True\n",
        "      clause = 'like'\n",
        "    else:\n",
        "      col = where[0]\n",
        "      op = where[1]\n",
        "      val = where[2]\n",
        "    if wclause:\n",
        "      return col, clause, val, True\n",
        "    else:\n",
        "      return col, op, val, False\n",
        "\n",
        "def where_and_or(where_clause):\n",
        "  where = str(where_clause).split(' ')\n",
        "  clause = ''\n",
        "  if 'and' in where_clause:\n",
        "      col1 = where[0]\n",
        "      op1 = where[1]\n",
        "      val1 = where[2]\n",
        "      col2 = where[4]\n",
        "      op2 = where[5]\n",
        "      val2 = where[6]\n",
        "      wclause = True\n",
        "      clause = 'and'\n",
        "  elif 'and' or 'in' in where_clause:\n",
        "      col1 = where[0]\n",
        "      op1 = where[1]\n",
        "      val1 = where[2]\n",
        "      col2 = where[4]\n",
        "      op2 = where[5]\n",
        "      val2 = where[6]\n",
        "      wclause = True\n",
        "      clause = 'or'\n",
        "  return col1, op1, val1, col2, op2, val2, clause\n",
        "\n",
        "def order(order_by_clause):\n",
        "    order = str(order_by_clause).split(' ')\n",
        "    col = order[0]\n",
        "    order_by = order[1]\n",
        "    return col, order_by\n",
        "\n",
        "def having(having_clause):\n",
        "    having = str(having_clause).split(' ')\n",
        "    col = having[0]\n",
        "    op = having[1]\n",
        "    val = having[2]\n",
        "    return col, op, val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvZYXRwA0OaC"
      },
      "outputs": [],
      "source": [
        "class select_parser:\n",
        "    def __init__(self, query):\n",
        "        self.query = query.strip().rstrip(';')\n",
        "        self.select_clause = None\n",
        "        self.from_clause = None\n",
        "        self.where_clause = None\n",
        "        self.order_by_clause = None\n",
        "        self.having_clause = None\n",
        "        self.join_clause = None\n",
        "        self.on_clause = None\n",
        "        self.group_by_clause = None\n",
        "\n",
        "    def parse(self):\n",
        "        self.select_clause = self._parse_clause(\"SELECT\", \"FROM\")\n",
        "        self.from_clause = self._parse_clause(\"FROM\", \"WHERE|JOIN|GROUP BY|ORDER BY|HAVING|$\")\n",
        "        self.where_clause = self._parse_clause(\"WHERE\", \"GROUP BY|ORDER BY|HAVING|JOIN|$\")\n",
        "        self.group_by_clause = self._parse_clause(\"GROUP BY\", \"HAVING|ORDER BY|$\")\n",
        "        self.order_by_clause = self._parse_clause(\"ORDER BY\", \"HAVING|$\")\n",
        "        self.having_clause = self._parse_clause(\"HAVING\", \"$\")\n",
        "        self.join_clause = self._parse_clause(\"JOIN\", \"ON\")\n",
        "        self.on_clause = self._parse_clause(\"ON\", \"WHERE|GROUP BY|ORDER BY|HAVING|$\")\n",
        "\n",
        "    def _parse_clause(self, start_keyword, end_keyword_pattern):\n",
        "        pattern = rf\"{start_keyword}\\s+(.*?)(?=(?:{end_keyword_pattern})|\\Z)\"\n",
        "        if isinstance(self.query, str):\n",
        "            match = re.search(pattern, self.query, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1).strip()\n",
        "        return None\n",
        "\n",
        "    def display_parsed_components(self):\n",
        "        and_or = False\n",
        "        cols, aggr = columns(self.select_clause)\n",
        "        # cols -> columns of the table\n",
        "        # aggr -> aggregations if exists else None\n",
        "        #print(\"Columns:\", cols)\n",
        "        #print(\"Aggregate Function:\", aggr)\n",
        "        table, limit = from_condition(self.from_clause)\n",
        "        # table -> table name\n",
        "        # limit -> limit if exists else None\n",
        "        #print(\"Table:\", table)\n",
        "        #print(\"Limit:\", limit)\n",
        "        if self.where_clause:\n",
        "          if 'and' in self.where_clause or 'or' in self.where_clause:\n",
        "            col1, op1, val1, col2, op2, val2, clause = where_and_or(self.where_clause)\n",
        "            #print(\"Column 1:\", col1)\n",
        "            #print(\"Operation 1:\", op1)\n",
        "            #print(\"Value 1:\", val1)\n",
        "            #print(\"Column 2:\", col2)\n",
        "            #print(\"Operation 2:\", op2)\n",
        "            #print(\"Value 2:\", val2)\n",
        "            #print(\"Clause:\", clause)\n",
        "            and_or = True\n",
        "          else:\n",
        "            col, cond, val, flag = where_condition(self.where_clause)\n",
        "            #print(\"Column:\", col)\n",
        "            #print(\"Condition:\", cond)\n",
        "            #print(\"Value:\", val)\n",
        "        if self.order_by_clause:\n",
        "          col, ord = order(self.order_by_clause)\n",
        "          #print(\"Column:\", col)\n",
        "          #print(\"Order:\", ord)\n",
        "        if self.having_clause:\n",
        "          col, op, val = having(self.having_clause)\n",
        "          #print(\"Column:\", col)\n",
        "          #print(\"Operation:\", op)\n",
        "          #print(\"Value:\", val)\n",
        "        #print(\"GROUP BY Clause:\", self.group_by_clause)\n",
        "        group_by = self.group_by_clause\n",
        "        #print(\"JOIN Clause:\", self.join_clause)\n",
        "        join = self.join_clause\n",
        "        #print(\"ON Clause:\", self.on_clause)\n",
        "        on = self.on_clause\n",
        "        if self.select_clause and self.from_clause and aggr != None:\n",
        "        # *************** Create a function to implement select with from - use cols, aggr, table, limit (Note: If there is no limit in the query this will be none, else will be a number)\n",
        "        # Should support the following queries :\n",
        "        # SELECT COUNT(*) FROM employees;\n",
        "        # SELECT SUM(id) FROM employees;\n",
        "        # SELECT AVG(id) FROM employees;\n",
        "        # SELECT MAX(id) FROM users;\n",
        "        # SELECT MIN(id) FROM employees;\n",
        "          #print('Calling select aggregation clause function')\n",
        "          if aggr == \"count\":\n",
        "              res = col_count(databases, table, cols)\n",
        "          elif aggr == \"avg\":\n",
        "              res = col_average(databases, table, cols)\n",
        "          elif aggr == \"max\":\n",
        "              res = col_max(databases, table, cols)\n",
        "          elif aggr == \"min\":\n",
        "              res = col_min(databases, table, cols)\n",
        "          elif aggr == \"sum\":\n",
        "              res = col_sum(databases, table, cols)\n",
        "          h = str(aggr)+'('+str(cols)+')'\n",
        "          print(tabulate([[res]], headers=[h], tablefmt=\"grid\"))\n",
        "\n",
        "        elif self.select_clause and self.from_clause and not self.join_clause and not self.where_clause and not self.order_by_clause and not self.group_by_clause:\n",
        "          # *************** Create a function to implement select with from - use cols, table, limit\n",
        "          # Note: If there is no limit in the query this will be none, else will be a number\n",
        "          # Should support the following queries :\n",
        "          # SELECT * FROM employees;\n",
        "          # SELECT first_name, last_name FROM employees;\n",
        "          # SELECT * FROM employees LIMIT 10;\n",
        "          #print('Calling simple select clause function')\n",
        "          simple_select(databases, table, cols, limit)\n",
        "\n",
        "        if self.select_clause and self.from_clause and self.join_clause:\n",
        "          #print('Calling select join clause function')\n",
        "          on = str(on).split('=')\n",
        "          cond1 = str(on[0]).replace(' ', '').split('.')\n",
        "          cond1 = cond1[1]\n",
        "          cond2 = str(on[1]).replace(' ', '').split('.')\n",
        "          cond2 = cond2[1]\n",
        "          joined_data, headers = optimised_join(databases[table], databases[join], cond1, cond2)\n",
        "          print(tabulate(joined_data, headers=headers, tablefmt='grid'))\n",
        "\n",
        "        if self.select_clause and self.from_clause and self.where_clause and and_or == True:\n",
        "          # *************** Create a function to implement select with 'and' or 'or' clause - use cols, table, col1, op1, val1, col2, op2, val2, clause as parameters\n",
        "          # Clause -> decides 'and' or 'or' clause\n",
        "          # Should support the following queries :\n",
        "          # SELECT * FROM users WHERE name = 'John' AND age = 25;\n",
        "          # SELECT * FROM users WHERE name = 'John' OR age = 25;\n",
        "          #print('Calling and or clause function')\n",
        "          select_with_and_or(databases, table, cols, col1, op1, val1, col2, op2, val2, clause)\n",
        "\n",
        "        elif self.select_clause and self.from_clause and self.where_clause:\n",
        "          # *************** Create a function too implement select with 'in', 'not in', 'like' or simple 'where' clause - use cols, table, col, cond, val and flag as parameters\n",
        "          # if flag == True -> cond will be 'in', 'not in' or 'like' clause\n",
        "          # if flag == False -> cond will be operation (>,<,=)\n",
        "          # Should support the following queries :\n",
        "          # SELECT * FROM employees WHERE name = 'Sales';\n",
        "          # SELECT * FROM users WHERE name in ('John' 'Jane');\n",
        "          # SELECT * FROM users WHERE name not in ('John' 'Jane');\n",
        "          # SELECT * FROM users WHERE name LIKE '%ab%';\n",
        "          #print('Calling in, not in, like clause function')\n",
        "          select_where(databases, table, cols, col, cond, val)\n",
        "\n",
        "        if self.select_clause and self.from_clause and self.order_by_clause:\n",
        "          # *************** Create a function too implement select with 'order' clause - use cols, table, col, ord as parameters\n",
        "          # Should support the following queries :\n",
        "          # SELECT * FROM employees ORDER BY name DESC;\n",
        "          # SELECT * FROM employees ORDER BY name ASC;\n",
        "          #print('Calling select order by clause function')\n",
        "          order_by(databases, table, cols, col, ord)\n",
        "\n",
        "        elif self.select_clause and self.from_clause and self.group_by_clause:\n",
        "          # *************** Create a function too implement select with 'group' clause - use cols, table, group_by as parameters\n",
        "          # Should support the following queries :\n",
        "          # SELECT id FROM employees GROUP BY name;\n",
        "          #print('Calling select group by clause function')\n",
        "          select_group_by(databases, table, cols, group_by)\n",
        "        return\n",
        "\n",
        "# Select table query parse implementation\n",
        "def select(command, pos):\n",
        "  cmd = str(command)\n",
        "  parser = select_parser(cmd)\n",
        "  parser.parse()\n",
        "  parser.display_parsed_components()\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alter functions"
      ],
      "metadata": {
        "id": "xDZEkW8YM5R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_column_names(database):\n",
        "    for table_name, table_info in database.items():\n",
        "        headers = table_info['headers']\n",
        "        print(f\"Table '{table_name}' columns: {headers}\")\n",
        "\n",
        "def add_column(database, table_name, column_name):\n",
        "    if table_name not in database:\n",
        "        print(f\"Table '{table_name}' not found.\")\n",
        "\n",
        "    # Access the table\n",
        "    table_dict = database[table_name]\n",
        "    headers = table_dict['headers']\n",
        "    data = table_dict['data']\n",
        "\n",
        "    # Check if the column already exists\n",
        "    if column_name in headers:\n",
        "        print(f\"Column '{column_name}' already exists in the table.\")\n",
        "\n",
        "    # Add the new column to headers\n",
        "    headers.append(column_name)\n",
        "    print(headers)\n",
        "\n",
        "    # Append a None value to each row in the data\n",
        "    for row in data:\n",
        "        row.append(None)\n",
        "\n",
        "    print(f\"Column '{column_name}' added successfully.\")\n",
        "    print_column_names(database)\n",
        "\n",
        "\n",
        "def drop_column(database, table_name, column_name):\n",
        "    if table_name not in database:\n",
        "        print(f\"Table '{table_name}' not found.\")\n",
        "\n",
        "    # Access the table\n",
        "    table_dict = database[table_name]\n",
        "    headers = table_dict['headers']\n",
        "    data = table_dict['data']\n",
        "\n",
        "    # Check if the column exists\n",
        "    if column_name not in headers:\n",
        "        print(f\"Column '{column_name}' does not exist in the table.\")\n",
        "\n",
        "    # Find the index of the column to be removed\n",
        "    column_index = headers.index(column_name)\n",
        "\n",
        "    # Remove the column from headers\n",
        "    headers.pop(column_index)\n",
        "\n",
        "    # Remove the corresponding data from each row\n",
        "    for row in data:\n",
        "        row.pop(column_index)\n",
        "\n",
        "    print(f\"Column '{column_name}' dropped successfully.\")\n",
        "    print_column_names(database)\n"
      ],
      "metadata": {
        "id": "bxjslXWOM7UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZqnLBr20OwF"
      },
      "source": [
        "# Alter Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xUBFgeV0PGA"
      },
      "outputs": [],
      "source": [
        "# Alter table query parse implementation\n",
        "def alter(command, pos):\n",
        "  tables = [] # Stores table names\n",
        "  values = [] # Stores values that have to be altered\n",
        "  op = '' # Stores operatio to be performed in alter query\n",
        "  cmd = str(command).split()\n",
        "  if pos['alter'] == 0 and pos['table'] == 1 and 'add' in pos:\n",
        "    op = 'alter add'\n",
        "    tables = cmd[2]\n",
        "    values = cmd[4:len(cmd)]\n",
        "    values = clean(values)\n",
        "    values = format(values)\n",
        "    if tables not in db.keys(): # Checking if the table exists\n",
        "      print('Table not found')\n",
        "    else: # Altering data - add\n",
        "      # *************** Alter add table function implementation - use op(string), tables(list) and values(dict) as parameters\n",
        "      #print('Calling alter add function')\n",
        "      values = list(values.keys())\n",
        "      add_column(databases, tables, values[0])\n",
        "  elif pos['alter'] == 0 and pos['table'] == 1 and 'drop' in pos and 'column' in pos:\n",
        "    op = 'alter drop'\n",
        "    tables = cmd[2]\n",
        "    values = cmd[5:len(cmd)]\n",
        "    values = clean(values)\n",
        "    print(values)\n",
        "    if tables not in db.keys(): # Checking if the table exists\n",
        "      print('Table not found')\n",
        "    elif not values not in db[tables]: # Checking if the columns exists\n",
        "      print('Column not found')\n",
        "    else: # Altering data - drop\n",
        "      # *************** Alter drop table function implementation - use op(string), tables(list) and values(dict) as parameters\n",
        "      #print('Calling alter drop function')\n",
        "      drop_column(databases, tables, values[0])\n",
        "  else:\n",
        "    print('Invalid Query')\n",
        "    # drop_table(databases, tables)\n",
        "  return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkcEb3jU7zJ1"
      },
      "source": [
        "# Show Tables - Not Select *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Z9uzXmZ7y1z"
      },
      "outputs": [],
      "source": [
        "# Show tables query implementation\n",
        "def show_all_tables(db):\n",
        "  table_names = [[name] for name in db.keys()]\n",
        "  print(tabulate(table_names, headers=[\"Tables\"], tablefmt=\"grid\"))\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW5o9KZsw363"
      },
      "source": [
        "# Main code for parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02WZ4qN8w0kW"
      },
      "outputs": [],
      "source": [
        "# Main function to call all operations\n",
        "def main(cmd, dynamic_db):\n",
        "  cmd = cmd.strip().lower() # Input command in lowercase\n",
        "  command = sqlparse.parse(cmd) # Parsing input query\n",
        "  pos = positions(command) # Getting positions of tokens in parsed query\n",
        "  command = str(command[0])\n",
        "  if \"create table\" in cmd:\n",
        "    create(command, pos) # Create table\n",
        "  elif \"insert into\" in cmd:\n",
        "    insert(command, pos) # Insert into table\n",
        "  elif \"select\" in cmd:\n",
        "    select(command, pos) # Select from table\n",
        "  elif \"update\" in cmd:\n",
        "    update(command, pos) # Update table\n",
        "  elif \"alter\" in cmd:\n",
        "    alter(command, pos) # Alter table\n",
        "  elif \"delete\" in cmd:\n",
        "    delete(command, pos) # Delete table\n",
        "  elif \"drop\" in cmd:\n",
        "    drop(command, pos) # Drop table\n",
        "  elif \"show tables\" in cmd:\n",
        "    show_all_tables(db) # Show tables\n",
        "  else:\n",
        "    print(\"Invalid command\") # Invalid input\n",
        "  return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn90kAE0w-Ye"
      },
      "source": [
        "# Command Line Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4Ih0mOlcxBIB",
        "outputId": "7018f6df-83d2-450f-adf8-a7f539758636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sql> update data_1 set col1 = 10 where col1 = 3;\n",
            "Updated 1 row(s) where col1 = 3 setting col1 to 10.\n",
            "(0.00) sec\n",
            "\n",
            "sql> select * from data_1 limit 4;\n",
            "+--------+--------+\n",
            "|   col1 |   col2 |\n",
            "+========+========+\n",
            "|      1 |      1 |\n",
            "+--------+--------+\n",
            "|      2 |      2 |\n",
            "+--------+--------+\n",
            "|     10 |      3 |\n",
            "+--------+--------+\n",
            "|      4 |      4 |\n",
            "+--------+--------+\n",
            "(0.00) sec\n",
            "\n",
            "sql> update data_1 set col1 = 100 where col1 = 10;\n",
            "Updated 1 row(s) where col1 = 10 setting col1 to 100.\n",
            "(0.00) sec\n",
            "\n",
            "sql> select * from data_1 limit 4;\n",
            "+--------+--------+\n",
            "|   col1 |   col2 |\n",
            "+========+========+\n",
            "|      1 |      1 |\n",
            "+--------+--------+\n",
            "|      2 |      2 |\n",
            "+--------+--------+\n",
            "|     10 |      3 |\n",
            "+--------+--------+\n",
            "|      4 |      4 |\n",
            "+--------+--------+\n",
            "(0.01) sec\n",
            "\n",
            "sql> update data_1 set col1 = 1000 where col1 = 10;\n",
            "Updated 1 row(s) where col1 = 10 setting col1 to 1000.\n",
            "(0.00) sec\n",
            "\n",
            "sql> select * from data_1 limit 4;\n",
            "+--------+--------+\n",
            "|   col1 |   col2 |\n",
            "+========+========+\n",
            "|      1 |      1 |\n",
            "+--------+--------+\n",
            "|      2 |      2 |\n",
            "+--------+--------+\n",
            "|   1000 |      3 |\n",
            "+--------+--------+\n",
            "|      4 |      4 |\n",
            "+--------+--------+\n",
            "(0.00) sec\n",
            "\n",
            "sql> update data_1 set col1 = 1000 where col2 = 1;\n",
            "Indexing field 'col2' because it was needed.\n",
            "Updated 1 row(s) where col2 = 1 setting col1 to 1000.\n",
            "(0.01) sec\n",
            "\n",
            "sql> select * from data_1 limit 4;\n",
            "+--------+--------+\n",
            "|   col1 |   col2 |\n",
            "+========+========+\n",
            "|   1000 |      1 |\n",
            "+--------+--------+\n",
            "|      2 |      2 |\n",
            "+--------+--------+\n",
            "|   1000 |      3 |\n",
            "+--------+--------+\n",
            "|      4 |      4 |\n",
            "+--------+--------+\n",
            "(0.00) sec\n",
            "\n",
            "sql> update data_1 set col1 = 5 where col2 = 1;\n",
            "Updated 1 row(s) where col2 = 1 setting col1 to 5.\n",
            "(0.00) sec\n",
            "\n",
            "sql> select * from data_1 limit 4;\n",
            "+--------+--------+\n",
            "|   col1 |   col2 |\n",
            "+========+========+\n",
            "|      5 |      1 |\n",
            "+--------+--------+\n",
            "|      2 |      2 |\n",
            "+--------+--------+\n",
            "|   1000 |      3 |\n",
            "+--------+--------+\n",
            "|      4 |      4 |\n",
            "+--------+--------+\n",
            "(0.00) sec\n",
            "\n",
            "sql> update data_1 set col2 = 5 where col1 = 5;\n",
            "Updated 2 row(s) where col1 = 5 setting col2 to 5.\n",
            "(0.00) sec\n",
            "\n",
            "sql> select * from data_1 limit 4;\n",
            "+--------+--------+\n",
            "|   col1 |   col2 |\n",
            "+========+========+\n",
            "|      5 |      5 |\n",
            "+--------+--------+\n",
            "|      2 |      2 |\n",
            "+--------+--------+\n",
            "|   1000 |      3 |\n",
            "+--------+--------+\n",
            "|      4 |      4 |\n",
            "+--------+--------+\n",
            "(0.00) sec\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-03e94ade75d9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sql> \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Getting input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Start of execution time of query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit;\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Exit command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Calling main function\n",
        "if __name__ == \"__main__\":\n",
        "    databases, file_paths = given_sample_upload()\n",
        "    # output will be first 10 rows of each database\n",
        "    dynamic_db = DynamicDB(databases)  # Assuming 'databases' is your data loaded from CSVs or similar.\n",
        "    dynamic_db.load_indices()  # This method should initialize the indices.\n",
        "\n",
        "\n",
        "    while True:\n",
        "        cmd = input(\"sql> \") # Getting input\n",
        "        st = time.time() # Start of execution time of query\n",
        "        if cmd.lower() == \"exit;\": # Exit command\n",
        "            save_databases_to_csv(databases, file_paths) # Change with load data function ********\n",
        "            break\n",
        "        main(cmd, dynamic_db) # Calling main to perform operations\n",
        "        et = time.time() # End of execution time of query\n",
        "        dur = et - st # Execution time of query\n",
        "        print(f\"({dur:.2f}) sec\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "update data_1 set col1 = 10 where col1 = 3;"
      ],
      "metadata": {
        "id": "FEoH5zlGVXSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Queries"
      ],
      "metadata": {
        "id": "0guxic7Ot6Qv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Queries:\n",
        "show tables;\n",
        "\n",
        "1. Create:\n",
        "create table if not exists parks (pid integer primary key, location varchar(10));\n",
        "\n",
        "2. Selects:\n",
        "\n",
        "select * from visitors;\n",
        "\n",
        "select * from rides limit 4;\n",
        "\n",
        "select max(age) from visitors;\n",
        "\n",
        "select min(age) from visitors;\n",
        "\n",
        "select sum(times_ridden) from rides;\n",
        "\n",
        "select max(times_ridden) from rides;\n",
        "\n",
        "select min(times_ridden) from rides;\n",
        "\n",
        "select avg(times_ridden) from rides;\n",
        "\n",
        "select count(ride_name) from rides;\n",
        "\n",
        "select * from visitors join rides on visitors.visitor_id = rides.visitor_id;\n",
        "\n",
        "select * from data_1 join data_2 on data_1.col1 = data_2.col1;\n",
        "\n",
        "select * from data_3 join data_4 on data_3.col1 = data_4.col1;\n",
        "\n",
        "3. Inserts:\n",
        "insert into visitors values (6, 'Sophia', 6, 'Elsa');\n",
        "\n",
        "insert into rides values (317, 6, 'ElsaPalaceTour', 3);\n",
        "\n",
        "\n",
        "4. Selects after inserts:\n",
        "select ride_name from rides where ride_id = 305;\n",
        "\n",
        "select age from visitors order by age asc;\n",
        "\n",
        "select visitor_id from visitors order by visitor_id asc;\n",
        "\n",
        "select age from visitors order by age desc;\n",
        "\n",
        "select visitor_id from visitors order by visitor_id desc;\n",
        "\n",
        "select ride_name, visitor_id from rides group by visitor_id;\n",
        "\n",
        "select ride_name, times_ridden from rides where times_ridden >\n",
        "2 or ride_name = 'AmazingSpiderRide';\n",
        "\n",
        "select ride_name, times_ridden from rides where times_ridden = 1 or ride_name = 'AmazingSpiderRide';\n",
        "\n",
        "5. Updates: -\n",
        "update visitors set age = 10 where visitor_id = 3;\n",
        "\n",
        "update visitors set age = 8 where visitor_id = 5;\n",
        "\n",
        "6. Alters:\n",
        "alter table rides add park_name varchar(20);\n",
        "\n",
        "alter table rides add park_status varchar(20);\n",
        "\n",
        "alter table rides drop column park_name;\n",
        "\n",
        "alter table rides drop column park_status;\n",
        "\n",
        "7. Delete:\n",
        "delete from rides where ride_id = 309;\n",
        "\n",
        "8. Drop:\n",
        "drop table parks;\n"
      ],
      "metadata": {
        "id": "bOIaSEwet9J_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "show tables;\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS users (id INT PRIMARY KEY, name TEXT, age INT, dept varchar(40));\n",
        "\n",
        "DROP TABLE data_4;\n",
        "\n",
        "SELECT * from data_1 LIMIT 10;\n",
        "\n",
        "SELECT col1 from data_1 order by col1 DESC;\n",
        "\n",
        "SELECT col1, col2 from data_1 by col1 DESC;\n",
        "\n",
        "ALTER TABLE data_1 ADD col3 VARCHAR(255);\n",
        "\n",
        "SELECT AVG(col1) FROM data_1;\n",
        "\n",
        "ALTER TABLE data_1 DROP COLUMN col3; <- GIVES ERRORS"
      ],
      "metadata": {
        "id": "3qOKZR2AJxmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KEIA3uB1ufvP"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}